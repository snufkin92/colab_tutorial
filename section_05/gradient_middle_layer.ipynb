{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snufkin92/colab_tutorial/blob/master/section_05/gradient_middle_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA_9EEatRRfm"
      },
      "source": [
        "# 中間層の勾配\n",
        "中間層における勾配を導出しましょう。  \n",
        "今回は回帰、分類ともに活性化関数にシグモイド関数を使うので、中間層における勾配の求め方は共通です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Keyt5q0VRRfp"
      },
      "source": [
        "## ●添字とニューロン数\n",
        "前回と同じく、次の表のように各層におけるニューロンの添字とニューロン数を設定します。\n",
        "\n",
        "||||\n",
        "|:-:|:-:|:--|\n",
        "| 層 | ニューロンの添字 | ニューロン数 |\n",
        "| 入力層 | $i$ | $l$ |\n",
        "| 中間層 | $j$ | $m$ |\n",
        "| 出力層 | $k$ | $n$ |\n",
        "||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS62skkwRRfq"
      },
      "source": [
        "## ●中間層の勾配\n",
        "中間層における、重みとバイアスの勾配を求めます。  \n",
        "$w_{ij}$を中間層の重み、$b_j$をバイアス、$u_j$を重みと入力の積にバイアスを加えた値とします。  \n",
        "また、入力層の出力を$y_i$とします。  \n",
        "これらを図で表すと、以下の図のようになります。\n",
        "\n",
        "**（図: 中間層における、単一ニューロンの順伝播）**\n",
        "<br>\n",
        "\n",
        "中間層では、出力層の場合と同様に次以下の関係が成り立ちます。  \n",
        "重みの勾配は$\\partial w_{ij}$と表記します。  \n",
        "\n",
        "（式 1）\n",
        "$$ \\partial w_{ij}=\\frac{\\partial E}{\\partial w_{ij}}=\\frac{\\partial E}{\\partial u_j}\\frac{\\partial u_j}{\\partial w_{ij}} $$\n",
        "\n",
        "ここで、右辺の$\\frac{\\partial u_j}{\\partial w_{ij}}$の部分は次のようになります。\n",
        "\n",
        "（式 2）\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial u_j}{\\partial w_{ij}} & = \\frac{\\partial (\\sum\\limits_{p=1}^ly_pw_{pj} + b_j)}{\\partial w_{ij}} \\\\\n",
        "& = \\frac{\\partial}{\\partial w_{ij}}(y_1w_{1j}+y_2w_{2j}+\\cdots +y_iw_{ij}+\\cdots + y_lw_{lj} + b_j) \\\\\n",
        "& = y_i\n",
        "\\end{aligned} $$\n",
        "\n",
        "これは出力層の場合と同様ですね。  \n",
        "添字の$p$は$\\Sigma$による総和のために便宜上使用しているだけなので、特に意味はありません。  \n",
        "\n",
        "次に、（式 1）の右辺の$\\frac{\\partial E}{\\partial u_j}$の部分ですが、連鎖律により次のようになります。\n",
        "\n",
        "（式 3）\n",
        "$$ \\frac{\\partial E}{\\partial u_j} = \\frac{\\partial E}{\\partial y_j}\\frac{\\partial y_j}{\\partial u_j} $$\n",
        "\n",
        "この式の右辺の$\\frac{\\partial y_j}{\\partial u_j}$の部分は、活性化関数の微分により求めることができます。   \n",
        "$\\frac{\\partial E}{\\partial y_j}$の部分は中間層の出力の勾配ですが、これは以前に出力層で求めた$\\partial y_j$ですね。  \n",
        "この$\\partial y_j$を用いて、（式 3）を以下のように$\\delta$として表します。\n",
        "\n",
        "（式 4）\n",
        "$$ \\delta_{j}=\\frac{\\partial E}{\\partial u_j} = \\partial y_j\\frac{\\partial y_j}{\\partial u_j} $$\n",
        "\n",
        "このように、$\\delta_{j}$を求めるためには、出力層で求めた$\\partial y_j$を使用します。  \n",
        "ニューラルネットワークを遡っていますね。\n",
        "\n",
        "（式 2）と（式 4）を用いると、（式 1）を次のように表すことができます。\n",
        "\n",
        "$$ \\partial w_{ij}=y_i\\delta_{j} $$\n",
        "\n",
        "出力層の場合と同様の、シンプルな形に重みの勾配をまとめることができました。\n",
        "\n",
        "それでは、バイアスの勾配を求めましょう。  \n",
        "これまでと同様に、バイアスの勾配$\\partial b_j$は連鎖律を用いて次のように表すことができます。\n",
        "\n",
        "（式 5）\n",
        "$$ \\partial b_j=\\frac{\\partial E}{\\partial b_j}=\\frac{\\partial E}{\\partial u_j}\\frac{\\partial u_j}{\\partial b_j} $$\n",
        "\n",
        "ここで、右辺の$\\frac{\\partial u_j}{\\partial b_j}$の部分は次のようになります。\n",
        "\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial u_j}{\\partial b_j} & = \\frac{\\partial (\\sum\\limits_{p=1}^l y_p w_{pj} + b_j)}{\\partial b_j} \\\\\n",
        "& = \\frac{\\partial}{\\partial b_j}(y_1w_{1j}+y_2w_{2j}+\\cdots +y_iw_{ij}+\\cdots + y_lw_{lj} + b_j) \\\\\n",
        "& = 1\n",
        "\\end{aligned} $$\n",
        "\n",
        "この式と（式 4）により、（式 5）を次のように表すことができます。\n",
        "\n",
        "$$ \\partial b_j=\\delta_{j} $$\n",
        "\n",
        "バイアスの勾配を求めることができました。  \n",
        "出力層と同じように、勾配は$\\delta_{j}$に等しくなります。  \n",
        "\n",
        "この層の上にさらに中間層がある場合は、以下のようにして$\\partial y_i$を求めて伝播させます。\n",
        "\n",
        "$$ \\partial y_i =  \\sum_{q=1}^m \\delta_q w_{iq} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOimhUgARRfr"
      },
      "source": [
        "## ●活性化関数の適用\n",
        "活性化関数にシグモイド関数を使い、$\\delta_{j}$を求めます。  \n",
        "$\\delta_{j}$を求めるためには、（式 4）を使用します。  \n",
        "\n",
        "（式 6）\n",
        "$$\\delta_{j} = \\partial y_j\\frac{\\partial y_j}{\\partial u_j}$$\n",
        "\n",
        "この式の$\\frac{\\partial y_j}{\\partial u_j}$ですが、活性化関数を偏微分して求めることができます。  \n",
        "中間層の活性化関数はシグモイド関数ですが、シグモイド関数$f(x)$の導関数は、\n",
        "\n",
        "$$f'(x)=(1-f(x))f(x)$$\n",
        "\n",
        "となります。  \n",
        "従って、$\\frac{\\partial y_j}{\\partial u_j}$は次のようになります。\n",
        "\n",
        "$$ \\begin{aligned} \\\\\n",
        "\\frac{\\partial y_j}{\\partial u_j} & = (1-y_j)y_j\n",
        "\\end{aligned} $$\n",
        "\n",
        "この式を（式 6）に代入すると、$\\delta_{j}$を次のように表すことができます。  \n",
        "\n",
        "$$ \\delta_{j} = \\partial y_j(1-y_j)y_j $$   \n",
        "\n",
        "$\\delta_{j}$を求めることができました。  \n",
        "この$\\delta_{j}$とこれまでに求めた各勾配の式を使って、中間層において計算すべき各勾配を次のようにまとめることができます。\n",
        "\n",
        "$$ \\delta_{j} = \\partial y_j(1-y_j)y_j $$\n",
        "$$ \\partial w_{ij}=y_i\\delta_{j} $$\n",
        "$$ \\partial b_j=\\delta_{j} $$\n",
        "$$ \\partial y_i =  \\sum_{q=1}^m \\delta_q w_{iq} $$\n",
        "\n",
        "以上により、中間層における各勾配を求めることができました。  \n",
        "回帰、分類ともに共通の方法で勾配を求めることができます。"
      ]
    }
  ]
}